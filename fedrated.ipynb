{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb1bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d3871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b9e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be273433",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b34c32bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Hi you just set your fleeb to level plumbus\n"
     ]
    }
   ],
   "source": [
    "log.info(\"Hi you just set your fleeb to level plumbus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "40b80b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class CenterServer:\n",
    "    def __init__(self, model, dataloader, device=\"cpu\"):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "\n",
    "    def aggregation(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def send_model(self):\n",
    "        return copy.deepcopy(self.model)\n",
    "\n",
    "    def validation(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FedAvgCenterServer(CenterServer):\n",
    "    def __init__(self, model, dataloader, device=\"cpu\"):\n",
    "        super().__init__(model, dataloader, device)\n",
    "\n",
    "    def aggregation(self, clients, aggregation_weights):\n",
    "        update_state = OrderedDict()\n",
    "\n",
    "        for k, client in enumerate(clients):\n",
    "            local_state = client.model.state_dict()\n",
    "            for key in self.model.state_dict().keys():\n",
    "                if k == 0:\n",
    "                    update_state[\n",
    "                        key] = local_state[key] * aggregation_weights[k]\n",
    "                else:\n",
    "                    update_state[\n",
    "                        key] += local_state[key] * aggregation_weights[k]\n",
    "\n",
    "        self.model.load_state_dict(update_state)\n",
    "\n",
    "    def validation(self, loss_fn):\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for img, target in self.dataloader:\n",
    "                img = img.to(self.device)\n",
    "                target = target.type(torch.LongTensor)\n",
    "                target = target.to(self.device)\n",
    "                logits = self.model(img)\n",
    "                test_loss += loss_fn(logits, target).item()\n",
    "                pred = logits.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        self.model.to(\"cpu\")\n",
    "        test_loss = test_loss / len(self.dataloader)\n",
    "        accuracy = 100. * correct / len(self.dataloader.dataset)\n",
    "\n",
    "        return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "02c02ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, dataloader, device='cpu'):\n",
    "        self.client_id = client_id\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self.__model = None\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.__model\n",
    "\n",
    "    @model.setter\n",
    "    def model(self, model):\n",
    "        self.__model = model\n",
    "\n",
    "    def client_update(self,  local_epoch, loss_fn):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader.dataset)\n",
    "\n",
    "\n",
    "class FedAvgClient(Client):\n",
    "    def client_update(self,  local_epoch, loss_fn):\n",
    "        self.model.train()\n",
    "        self.model.to(self.device)\n",
    "        optimizer = optim.Adam(self.model.parameters(),lr = 1e-3)\n",
    "        for i in range(local_epoch):\n",
    "            for img, target in self.dataloader:\n",
    "                img = img.to(self.device)\n",
    "                target = target.type(torch.LongTensor)\n",
    "                target = target.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(img)\n",
    "                loss = loss_fn(logits, target)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        self.model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77e02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MnistLocalDataset(Dataset):\n",
    "    def __init__(self, images, labels, client_id):\n",
    "        self.images = images\n",
    "        self.labels = labels.astype(int)\n",
    "        self.client_id = client_id\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.fromarray(self.images[index].reshape(28, 28), mode='L')\n",
    "        img = self.transform(img)\n",
    "        target = self.labels[index]\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9994713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4f3ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as ops\n",
    "import urllib.request\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_mnist_data(datadir):\n",
    "    dataroot = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    key_file = {\n",
    "        'train_img': 'train-images-idx3-ubyte.gz',\n",
    "        'train_label': 'train-labels-idx1-ubyte.gz',\n",
    "        'test_img': 't10k-images-idx3-ubyte.gz',\n",
    "        'test_label': 't10k-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "    os.makedirs(datadir, exist_ok=True)\n",
    "\n",
    "    for key, filename in key_file.items():\n",
    "        if ops.exists(ops.join(datadir, filename)):\n",
    "            print(f\"already downloaded : {filename}\")\n",
    "        else:\n",
    "            urllib.request.urlretrieve(ops.join(dataroot, filename),\n",
    "                                       ops.join(datadir, filename))\n",
    "\n",
    "    with gzip.open(ops.join(datadir, key_file[\"train_img\"]), \"rb\") as f:\n",
    "        train_img = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    train_img = train_img.reshape(-1, 784)\n",
    "\n",
    "    with gzip.open(ops.join(datadir, key_file[\"train_label\"]), \"rb\") as f:\n",
    "        train_label = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "\n",
    "    with gzip.open(ops.join(datadir, key_file[\"test_img\"]), \"rb\") as f:\n",
    "        test_img = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    test_img = test_img.reshape(-1, 784)\n",
    "\n",
    "    with gzip.open(ops.join(datadir, key_file[\"test_label\"]), \"rb\") as f:\n",
    "        test_label = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "\n",
    "    return train_img, train_label, test_img,  test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e58808",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedBase:\n",
    "    def create_mnist_datasets(self,\n",
    "                              num_clients=100,\n",
    "                              shard_size=300,\n",
    "                              datadir=\"./data/mnist\",\n",
    "                              iid=False):\n",
    "        train_img, train_label, test_img, test_label = get_mnist_data(datadir)\n",
    "\n",
    "        train_sorted_index = np.argsort(train_label)\n",
    "        train_img = train_img[train_sorted_index]\n",
    "        train_label = train_label[train_sorted_index]\n",
    "\n",
    "        if iid:\n",
    "            random.shuffle(train_sorted_index)\n",
    "            train_img = train_img[train_sorted_index]\n",
    "            train_label = train_label[train_sorted_index]\n",
    "\n",
    "        shard_start_index = [i for i in range(0, len(train_img), shard_size)]\n",
    "        random.shuffle(shard_start_index)\n",
    "        print(\n",
    "            f\"divide data into {len(shard_start_index)} shards of size {shard_size}\"\n",
    "        )\n",
    "\n",
    "        num_shards = len(shard_start_index) // num_clients\n",
    "        local_datasets = []\n",
    "        for client_id in range(num_clients):\n",
    "            _index = num_shards * client_id\n",
    "            img = np.concatenate([\n",
    "                train_img[shard_start_index[_index +\n",
    "                                            i]:shard_start_index[_index + i] +\n",
    "                          shard_size] for i in range(num_shards)\n",
    "            ],\n",
    "                                 axis=0)\n",
    "\n",
    "            label = np.concatenate([\n",
    "                train_label[shard_start_index[_index +\n",
    "                                              i]:shard_start_index[_index +\n",
    "                                                                   i] +\n",
    "                            shard_size] for i in range(num_shards)\n",
    "            ],\n",
    "                                   axis=0)\n",
    "\n",
    "            local_datasets.append(MnistLocalDataset(img, label, client_id))\n",
    "\n",
    "        test_sorted_index = np.argsort(test_label)\n",
    "        test_img = test_img[test_sorted_index]\n",
    "        test_label = test_label[test_sorted_index]\n",
    "\n",
    "        test_dataset = MnistLocalDataset(test_img, test_label, client_id=-1)\n",
    "\n",
    "        return local_datasets, test_dataset\n",
    "\n",
    "    def train_step(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def validation_step(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def fit(self, num_round):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5ca334f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already downloaded : train-images-idx3-ubyte.gz\n",
      "already downloaded : train-labels-idx1-ubyte.gz\n",
      "already downloaded : t10k-images-idx3-ubyte.gz\n",
      "already downloaded : t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "train_img, train_label, test_img, test_label = get_mnist_data(\"./data/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3aa6f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedAvg(FedBase):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 \n",
    "                 num_clients=200,\n",
    "                 batchsize=50,\n",
    "                 fraction=1,\n",
    "                 local_epoch=1,\n",
    "                 iid=False,\n",
    "                 device=\"cpu\",\n",
    "                 writer=None):\n",
    "        \n",
    "\n",
    "        self.num_clients = num_clients  # K\n",
    "        self.batchsize = batchsize  # B\n",
    "        self.fraction = fraction  # C, 0 < C <= 1\n",
    "        self.local_epoch = local_epoch  # E\n",
    "\n",
    "        local_datasets, test_dataset = self.create_mnist_datasets(\n",
    "            num_clients, shard_size=300, iid=iid)\n",
    "        local_dataloaders = [\n",
    "            DataLoader(dataset,\n",
    "                       num_workers=0,\n",
    "                       batch_size=batchsize,\n",
    "                       shuffle=True) for dataset in local_datasets\n",
    "        ]\n",
    "\n",
    "        self.clients = [\n",
    "            FedAvgClient(k, local_dataloaders[k], device) for k in range(num_clients)\n",
    "        ]\n",
    "        self.total_data_size = sum([len(client) for client in self.clients])\n",
    "        self.aggregation_weights = [\n",
    "            len(client) / self.total_data_size for client in self.clients\n",
    "        ]\n",
    "\n",
    "        test_dataloader = DataLoader(test_dataset,\n",
    "                                     num_workers=0,\n",
    "                                     batch_size=batchsize)\n",
    "        self.center_server = FedAvgCenterServer(model, test_dataloader, device)\n",
    "\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "\n",
    "        self.writer = writer\n",
    "\n",
    "        self._round = 0\n",
    "        self.result = None\n",
    "\n",
    "    def fit(self, num_round):\n",
    "        self._round = 0\n",
    "        self.result = {'loss': [], 'accuracy': []}\n",
    "        self.validation_step()\n",
    "        for t in range(num_round):\n",
    "            self._round = t + 1\n",
    "            self.train_step()\n",
    "            self.validation_step()\n",
    "\n",
    "    def train_step(self):\n",
    "        self.send_model()\n",
    "        n_sample = max(int(self.fraction * self.num_clients), 1)\n",
    "        sample_set = np.random.randint(0, self.num_clients, n_sample)\n",
    "        for k in iter(sample_set):\n",
    "            self.clients[k].client_update(\n",
    "                                          self.local_epoch, self.loss_fn)\n",
    "        self.center_server.aggregation(self.clients, self.aggregation_weights)\n",
    "\n",
    "    def send_model(self):\n",
    "        for client in self.clients:\n",
    "            client.model = self.center_server.send_model()\n",
    "\n",
    "    def validation_step(self):\n",
    "        test_loss, accuracy = self.center_server.validation(self.loss_fn)\n",
    "        log.info(\n",
    "            f\"[Round: {self._round: 04}] Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\"\n",
    "        )\n",
    "        if self.writer is not None:\n",
    "            self.writer.add_scalar(\"val/loss\", test_loss, self._round)\n",
    "            self.writer.add_scalar(\"val/accuracy\", accuracy, self._round)\n",
    "\n",
    "        self.result['loss'].append(test_loss)\n",
    "        self.result['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b49152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9d1c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "97884031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already downloaded : train-images-idx3-ubyte.gz\n",
      "already downloaded : train-labels-idx1-ubyte.gz\n",
      "already downloaded : t10k-images-idx3-ubyte.gz\n",
      "already downloaded : t10k-labels-idx1-ubyte.gz\n",
      "divide data into 200 shards of size 300\n"
     ]
    }
   ],
   "source": [
    "A=FedAvg(model,num_clients=50,\n",
    "                 batchsize=50,\n",
    "                 fraction=0.2,\n",
    "                 local_epoch=5,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4f6ed711",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FedAvg' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FedAvg' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "A.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2d9dc948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[Round:  000] Test set: Average loss: 2.2990, Accuracy: 16.12%\n",
      "INFO:__main__:[Round:  001] Test set: Average loss: 2.1980, Accuracy: 29.20%\n",
      "INFO:__main__:[Round:  002] Test set: Average loss: 2.0396, Accuracy: 48.08%\n",
      "INFO:__main__:[Round:  003] Test set: Average loss: 1.8784, Accuracy: 50.80%\n",
      "INFO:__main__:[Round:  004] Test set: Average loss: 1.6689, Accuracy: 53.40%\n",
      "INFO:__main__:[Round:  005] Test set: Average loss: 1.4759, Accuracy: 61.97%\n",
      "INFO:__main__:[Round:  006] Test set: Average loss: 1.2403, Accuracy: 69.47%\n",
      "INFO:__main__:[Round:  007] Test set: Average loss: 0.9801, Accuracy: 78.32%\n",
      "INFO:__main__:[Round:  008] Test set: Average loss: 0.7381, Accuracy: 91.71%\n",
      "INFO:__main__:[Round:  009] Test set: Average loss: 0.6387, Accuracy: 90.10%\n",
      "INFO:__main__:[Round:  010] Test set: Average loss: 0.5049, Accuracy: 92.55%\n",
      "INFO:__main__:[Round:  011] Test set: Average loss: 0.4487, Accuracy: 90.81%\n",
      "INFO:__main__:[Round:  012] Test set: Average loss: 0.3548, Accuracy: 93.36%\n",
      "INFO:__main__:[Round:  013] Test set: Average loss: 0.3038, Accuracy: 93.84%\n",
      "INFO:__main__:[Round:  014] Test set: Average loss: 0.2888, Accuracy: 93.39%\n",
      "INFO:__main__:[Round:  015] Test set: Average loss: 0.2814, Accuracy: 93.38%\n",
      "INFO:__main__:[Round:  016] Test set: Average loss: 0.2673, Accuracy: 93.53%\n",
      "INFO:__main__:[Round:  017] Test set: Average loss: 0.2627, Accuracy: 93.14%\n",
      "INFO:__main__:[Round:  018] Test set: Average loss: 0.2162, Accuracy: 94.58%\n",
      "INFO:__main__:[Round:  019] Test set: Average loss: 0.1991, Accuracy: 95.13%\n",
      "INFO:__main__:[Round:  020] Test set: Average loss: 0.1801, Accuracy: 95.63%\n",
      "INFO:__main__:[Round:  021] Test set: Average loss: 0.1733, Accuracy: 95.56%\n",
      "INFO:__main__:[Round:  022] Test set: Average loss: 0.1915, Accuracy: 94.65%\n",
      "INFO:__main__:[Round:  023] Test set: Average loss: 0.1872, Accuracy: 94.54%\n",
      "INFO:__main__:[Round:  024] Test set: Average loss: 0.1394, Accuracy: 96.30%\n",
      "INFO:__main__:[Round:  025] Test set: Average loss: 0.1332, Accuracy: 96.60%\n",
      "INFO:__main__:[Round:  026] Test set: Average loss: 0.1187, Accuracy: 97.00%\n",
      "INFO:__main__:[Round:  027] Test set: Average loss: 0.1139, Accuracy: 96.97%\n",
      "INFO:__main__:[Round:  028] Test set: Average loss: 0.1112, Accuracy: 97.02%\n",
      "INFO:__main__:[Round:  029] Test set: Average loss: 0.1089, Accuracy: 96.89%\n",
      "INFO:__main__:[Round:  030] Test set: Average loss: 0.1111, Accuracy: 96.90%\n",
      "INFO:__main__:[Round:  031] Test set: Average loss: 0.0986, Accuracy: 97.21%\n",
      "INFO:__main__:[Round:  032] Test set: Average loss: 0.0964, Accuracy: 97.23%\n",
      "INFO:__main__:[Round:  033] Test set: Average loss: 0.1047, Accuracy: 96.86%\n",
      "INFO:__main__:[Round:  034] Test set: Average loss: 0.0983, Accuracy: 97.10%\n",
      "INFO:__main__:[Round:  035] Test set: Average loss: 0.0909, Accuracy: 97.37%\n",
      "INFO:__main__:[Round:  036] Test set: Average loss: 0.0893, Accuracy: 97.39%\n",
      "INFO:__main__:[Round:  037] Test set: Average loss: 0.0880, Accuracy: 97.34%\n",
      "INFO:__main__:[Round:  038] Test set: Average loss: 0.0791, Accuracy: 97.57%\n",
      "INFO:__main__:[Round:  039] Test set: Average loss: 0.0901, Accuracy: 97.16%\n",
      "INFO:__main__:[Round:  040] Test set: Average loss: 0.1066, Accuracy: 96.63%\n",
      "INFO:__main__:[Round:  041] Test set: Average loss: 0.0966, Accuracy: 97.19%\n",
      "INFO:__main__:[Round:  042] Test set: Average loss: 0.1023, Accuracy: 96.83%\n",
      "INFO:__main__:[Round:  043] Test set: Average loss: 0.0990, Accuracy: 96.82%\n",
      "INFO:__main__:[Round:  044] Test set: Average loss: 0.0794, Accuracy: 97.67%\n",
      "INFO:__main__:[Round:  045] Test set: Average loss: 0.0702, Accuracy: 97.94%\n",
      "INFO:__main__:[Round:  046] Test set: Average loss: 0.0613, Accuracy: 98.24%\n",
      "INFO:__main__:[Round:  047] Test set: Average loss: 0.0643, Accuracy: 98.08%\n",
      "INFO:__main__:[Round:  048] Test set: Average loss: 0.0608, Accuracy: 98.04%\n",
      "INFO:__main__:[Round:  049] Test set: Average loss: 0.0648, Accuracy: 98.05%\n",
      "INFO:__main__:[Round:  050] Test set: Average loss: 0.0592, Accuracy: 98.13%\n"
     ]
    }
   ],
   "source": [
    "A.fit(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73d815ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_features=1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_features,\n",
    "                               32,\n",
    "                               kernel_size=5,\n",
    "                               padding=0,\n",
    "                               stride=1,\n",
    "                               bias=True)\n",
    "        self.conv2 = nn.Conv2d(32,\n",
    "                               64,\n",
    "                               kernel_size=5,\n",
    "                               padding=0,\n",
    "                               stride=1,\n",
    "                               bias=True)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dba68749",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed590be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (act): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f2b791b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "327ff18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095fe96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
